{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\github_clone\\\\gridGPT'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.gpt import gridGPT, gridGPTAgent\n",
    "import torch\n",
    "import grid2op\n",
    "from lightsim2grid import LightSimBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:03:09 | INFO     | Using ActionConverter with action size: 178\n",
      "\n",
      "=== TEST OUTPUT ===\n",
      "Raw discrete action ID: 30\n",
      "Converted grid action: This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - NOT modify any storage capacity\n",
      "\t - NOT perform any curtailment\n",
      "\t - NOT force any line status\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - Set the bus of the following element(s):\n",
      "\t \t - Assign bus 1 to line (extremity) id 0 [on substation 1]\n",
      "\t \t - Assign bus 2 to line (origin) id 2 [on substation 1]\n",
      "\t \t - Assign bus 2 to line (origin) id 3 [on substation 1]\n",
      "\t \t - Assign bus 2 to line (origin) id 4 [on substation 1]\n",
      "\t \t - Assign bus 1 to generator id 0 [on substation 1]\n",
      "\t \t - Assign bus 2 to load id 0 [on substation 1]\n",
      "Log prob: tensor([-5.6125], device='cuda:0')\n",
      "State value: tensor([0.5169], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = grid2op.make(\"l2rpn_case14_sandbox\",\n",
    "                    backend=LightSimBackend())\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'has_continuous_action_space': False,\n",
    "    'gamma': 0.99,\n",
    "    'eps_clip': 0.2,\n",
    "    'K_epochs': 4,\n",
    "    'lr_actor': 3e-5,\n",
    "}\n",
    "\n",
    "agent = gridGPTAgent(env, config)\n",
    "\n",
    "# ---- Dummy input tensors ----\n",
    "m = agent.policy  # gridGPT model\n",
    "\n",
    "B = 1\n",
    "L = m.config['context_len']\n",
    "state_dim = m.config['state_dim']\n",
    "action_size = m.config['action_size']\n",
    "\n",
    "prev_state = torch.randn(B, L, state_dim, device=device)\n",
    "next_state = torch.randn(B, L, state_dim, device=device)\n",
    "prev_action = torch.randint(low=0, high=action_size, size=(B, L), device=device)\n",
    "slot_idx = torch.arange(L, device=device).unsqueeze(0)\n",
    "timestep = torch.arange(100, 100 + L, device=device).unsqueeze(0)\n",
    "action_mask_last = torch.ones(B, action_size, dtype=torch.bool, device=device)\n",
    "\n",
    "# ---- Call select_action ----\n",
    "action, grid_action, logprob, value = agent.select_action(\n",
    "    prev_state, prev_action, next_state, slot_idx, timestep, action_mask_last\n",
    ")\n",
    "\n",
    "print(\"\\n=== TEST OUTPUT ===\")\n",
    "print(\"Raw discrete action ID:\", action)\n",
    "print(\"Converted grid action:\", grid_action)\n",
    "print(\"Log prob:\", logprob)\n",
    "print(\"State value:\", value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled action: [122, 95]\n",
      "logprob: [-5.1724042892456055, -5.187069892883301]\n",
      "value: [0.04453800246119499, 0.11552482098340988]\n",
      "greedy action: [25, 69]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "m = gridGPT().to(device).eval()\n",
    "\n",
    "L = m.config['context_len']\n",
    "B = 2\n",
    "state_dim   = m.config['state_dim']\n",
    "action_size = m.config['action_size']\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(123)\n",
    "prev_states  = torch.randn(B, L, state_dim, generator=g, device=device)\n",
    "next_states  = torch.randn(B, L, state_dim, generator=g, device=device)\n",
    "prev_actions = torch.randint(low=0, high=action_size, size=(B, L), generator=g, device=device)\n",
    "slot_idx     = torch.arange(L, device=device).unsqueeze(0).repeat(B, 1)\n",
    "t0 = 100\n",
    "timestep     = torch.arange(t0, t0 + L, device=device).unsqueeze(0).repeat(B, 1)\n",
    "action_mask_last = torch.ones(B, action_size, dtype=torch.bool, device=device)\n",
    "\n",
    "# stochastic action (exploration)\n",
    "a, a_logp, v = m.act(prev_states, prev_actions, next_states, slot_idx, timestep, action_mask_last, deterministic=False)\n",
    "print(\"sampled action:\", a.tolist())\n",
    "print(\"logprob:\", a_logp.tolist())\n",
    "print(\"value:\", v.tolist())\n",
    "\n",
    "# greedy action (evaluation)\n",
    "a_g, a_g_logp, v_g = m.act(prev_states, prev_actions, next_states, slot_idx, timestep, action_mask_last, deterministic=True)\n",
    "print(\"greedy action:\", a_g.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Probs (batch 0): [0.004006871487945318, 0.004645700100809336, 0.007731734309345484, 0.004200957249850035, 0.004426130559295416, 0.0059465281665325165, 0.0037149381823837757, 0.005040147807449102, 0.0053291283547878265, 0.005648881196975708, 0.004240559879690409, 0.007967649027705193, 0.00638725096359849, 0.007827120833098888, 0.006583447102457285, 0.006404843647032976, 0.007433420047163963, 0.00384664093144238, 0.004772510379552841, 0.005047444719821215, 0.006787172984331846, 0.0055165705271065235, 0.005603952798992395, 0.0046149552799761295, 0.005651440005749464, 0.005218531470745802, 0.007042560260742903, 0.004284872207790613, 0.0032555346842855215, 0.007129600737243891, 0.003655410371720791, 0.0053522237576544285, 0.0042580412700772285, 0.0055267103016376495, 0.005788374226540327, 0.00641618063673377, 0.00452744634822011, 0.005573666654527187, 0.005930819548666477, 0.004972686991095543, 0.005547208711504936, 0.007894421927630901, 0.0058663347736001015, 0.010178259573876858, 0.0046659670770168304, 0.005734679289162159, 0.005099914036691189, 0.004699843004345894, 0.007666586432605982, 0.0060187228955328465, 0.004137040581554174, 0.0054453834891319275, 0.00831754133105278, 0.006540781818330288, 0.005721391644328833, 0.00709273898974061, 0.007841911166906357, 0.0044624535366892815, 0.007323988247662783, 0.004368726164102554, 0.005821450147777796, 0.007192153483629227, 0.00570334168151021, 0.0051457383669912815, 0.0046584224328398705, 0.005027828738093376, 0.006025627721101046, 0.004255616571754217, 0.005633639637380838, 0.0056137205101549625, 0.005833667702972889, 0.0057981316931545734, 0.005320215597748756, 0.00593204190954566, 0.007059949450194836, 0.003986174240708351, 0.004996778443455696, 0.00467610964551568, 0.008521752431988716, 0.005431948229670525, 0.0048934160731732845, 0.003880946896970272, 0.006834502797573805, 0.006093679461628199, 0.005853068083524704, 0.006821735762059689, 0.006011276505887508, 0.005499435588717461, 0.004001590423285961, 0.004984678700566292, 0.00582675077021122, 0.005511514376848936, 0.0053718723356723785, 0.0076354313641786575, 0.005168893840163946, 0.007943978533148766, 0.004168712068349123, 0.007655631750822067, 0.004316273145377636, 0.007475640159100294, 0.00574893644079566, 0.006366754416376352, 0.006336170714348555, 0.006136502604931593, 0.0039409464225173, 0.005012016277760267, 0.006586052943021059, 0.004722801502794027, 0.004559564404189587, 0.003609428647905588, 0.0070798820815980434, 0.006697612348943949, 0.004742651246488094, 0.007782809901982546, 0.005867986008524895, 0.00693347305059433, 0.0070357597433030605, 0.006850328762084246, 0.003525996580719948, 0.007438782602548599, 0.006465893238782883, 0.006370800081640482, 0.004423086531460285, 0.005615063011646271, 0.0042490572668612, 0.005334336776286364, 0.007417304441332817, 0.007237633224576712, 0.005652750376611948, 0.006853712256997824, 0.006492316722869873, 0.005703684873878956, 0.0047980863600969315, 0.005381592083722353, 0.005040021613240242, 0.006386750843375921, 0.0042042918503284454, 0.0036524333991110325, 0.005526320077478886, 0.0031605323310941458, 0.0069724577479064465, 0.004664866253733635, 0.003801636165007949, 0.004466633312404156, 0.004770529922097921, 0.0051971133798360825, 0.006769487168639898, 0.004859586246311665, 0.004973098170012236, 0.006098712328821421, 0.005445403512567282, 0.0054718125611543655, 0.005117426626384258, 0.006952821742743254, 0.00493987649679184, 0.004396780394017696, 0.00706871971487999, 0.004332264419645071, 0.004622302483767271, 0.006752314977347851, 0.004977303557097912, 0.004159779753535986, 0.0051431781612336636, 0.006547980010509491, 0.0056262086145579815, 0.00525321438908577, 0.005068203900009394, 0.003836330259218812, 0.007027702406048775, 0.0038552756886929274, 0.003325534285977483, 0.007052637171000242, 0.0055375234223902225, 0.0044535002671182156, 0.0051313783042132854, 0.003611491061747074, 0.005556418560445309, 0.007706523407250643]\n",
      "Probs after history change (batch 0): [0.004008229356259108, 0.004650382790714502, 0.007737188134342432, 0.004206642042845488, 0.004428228363394737, 0.005948846694082022, 0.003716227598488331, 0.005036665592342615, 0.005323648918420076, 0.005660626571625471, 0.004235681612044573, 0.007970131933689117, 0.006386872380971909, 0.007831602357327938, 0.0065836128778755665, 0.006406537257134914, 0.00743831554427743, 0.003846353618428111, 0.004774342756718397, 0.005050009116530418, 0.006788516417145729, 0.00551516143605113, 0.005608922336250544, 0.004614448640495539, 0.00565768638625741, 0.00521492213010788, 0.007036319002509117, 0.004281914792954922, 0.003252422669902444, 0.007130864076316357, 0.0036496964748948812, 0.005353017710149288, 0.0042563132010400295, 0.005520425271242857, 0.005788103677332401, 0.006407065317034721, 0.004525969736278057, 0.0055719115771353245, 0.005917295813560486, 0.004969968926161528, 0.0055434200912714005, 0.007895163260400295, 0.005873410496860743, 0.01017402857542038, 0.004670789930969477, 0.005734161473810673, 0.005105303134769201, 0.0046960944309830666, 0.007663761731237173, 0.006020621862262487, 0.0041342610493302345, 0.005434324033558369, 0.008319766260683537, 0.006556922569870949, 0.00571481604129076, 0.007093673571944237, 0.00784141756594181, 0.004465748090296984, 0.007322566583752632, 0.004365998785942793, 0.005821730010211468, 0.007197166793048382, 0.005698178429156542, 0.0051479400135576725, 0.004664937034249306, 0.005026227328926325, 0.006021786481142044, 0.0042558703571558, 0.0056384410709142685, 0.0056222607381641865, 0.005831381771713495, 0.005800901912152767, 0.005311288870871067, 0.005933153443038464, 0.007053934037685394, 0.003991575911641121, 0.005003150552511215, 0.004685493186116219, 0.008509495295584202, 0.005434732884168625, 0.004890859592705965, 0.0038850002456456423, 0.006831664126366377, 0.006094837095588446, 0.005853773560374975, 0.00681873457506299, 0.0060049062594771385, 0.005498675629496574, 0.0039980849251151085, 0.00498303072527051, 0.005824713967740536, 0.005513712298125029, 0.005365309305489063, 0.007636137306690216, 0.005163500085473061, 0.00794613640755415, 0.004165278282016516, 0.00765611557289958, 0.004318598657846451, 0.007485098205506802, 0.005750866606831551, 0.006359889172017574, 0.006325257942080498, 0.006133065093308687, 0.00394496601074934, 0.005014490336179733, 0.006585387513041496, 0.004728447180241346, 0.004561422858387232, 0.0036082882434129715, 0.00708273658528924, 0.00669217761605978, 0.004737196955829859, 0.007785073947161436, 0.005875840317457914, 0.006932515185326338, 0.007039482239633799, 0.006845153868198395, 0.003522804006934166, 0.00744366180151701, 0.006470225285738707, 0.00637283269315958, 0.004430864471942186, 0.005616104230284691, 0.004248843062669039, 0.005328064784407616, 0.007416557054966688, 0.007241072133183479, 0.005647527053952217, 0.0068540628999471664, 0.006491351872682571, 0.005698270630091429, 0.00480047520250082, 0.005382966715842485, 0.005036455113440752, 0.00638192892074585, 0.004205560311675072, 0.0036542315501719713, 0.005536484997719526, 0.0031598976347595453, 0.006975074764341116, 0.0046560619957745075, 0.003804595908150077, 0.004468933213502169, 0.004769838415086269, 0.005206584930419922, 0.0067642852663993835, 0.004857663530856371, 0.004971576388925314, 0.006097003351897001, 0.005443249363452196, 0.005464653018862009, 0.0051163421012461185, 0.006961303297430277, 0.0049441298469901085, 0.004393293056637049, 0.007070822641253471, 0.0043324814178049564, 0.004619852174073458, 0.006746635306626558, 0.0049765342846512794, 0.004160665906965733, 0.005143755581229925, 0.0065572974272072315, 0.005634634755551815, 0.005245266482234001, 0.005070752464234829, 0.0038347062654793262, 0.007024961523711681, 0.0038544752169400454, 0.0033262225333601236, 0.00705626280978322, 0.0055311210453510284, 0.004455001093447208, 0.005133668426424265, 0.003612903179600835, 0.005555330775678158, 0.007708409801125526]\n",
      "Max abs change in probs (batch 0): 1.6140751540660858e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ========= CUDA / device setup =========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Optional: slightly faster matmul on Ampere+ GPUs\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----- 1) Instantiate model -----\n",
    "m = gridGPT()            # your class\n",
    "m.eval().to(device)      # move to GPU\n",
    "\n",
    "# Sanity: L must equal the context_len you configured\n",
    "L = m.config['context_len']\n",
    "B = 2\n",
    "state_dim   = m.config['state_dim']\n",
    "action_size = m.config['action_size']\n",
    "\n",
    "# ----- 2) Build a small window batch on the right device -----\n",
    "# Random but fixed for reproducibility\n",
    "g = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "prev_states   = torch.randn(B, L, state_dim, generator=g, device=device)\n",
    "next_states   = torch.randn(B, L, state_dim, generator=g, device=device)\n",
    "prev_actions  = torch.randint(low=0, high=action_size, size=(B, L), generator=g, device=device)\n",
    "\n",
    "# slot indices 0..L-1 per batch\n",
    "slot_idx = torch.arange(L, device=device).unsqueeze(0).repeat(B, 1)\n",
    "\n",
    "# absolute timesteps (e.g., 100..100+L-1). You can mod/clip if needed.\n",
    "t0 = 100\n",
    "timestep = torch.arange(t0, t0 + L, device=device).unsqueeze(0).repeat(B, 1)\n",
    "\n",
    "# Optional mask for last slot (all actions are valid here)\n",
    "action_mask_last = torch.ones(B, action_size, dtype=torch.bool, device=device)\n",
    "\n",
    "# ----- 3) Forward pass: get logits/probs for current decision (last slot) -----\n",
    "with torch.no_grad():\n",
    "    logits = m(prev_states, prev_actions, next_states, slot_idx, timestep, action_mask_last=action_mask_last)\n",
    "    probs  = F.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"Probs (batch 0):\", probs[0].tolist())\n",
    "\n",
    "# ----- 4) Show that HISTORY affects the decision -----\n",
    "# Keep the LAST slot identical, but perturb an EARLY slot in sample 0.\n",
    "prev_states_pert   = prev_states.clone()\n",
    "next_states_pert   = next_states.clone()\n",
    "prev_actions_pert  = prev_actions.clone()\n",
    "\n",
    "# Modify an early slot (e.g., slot 2) in sample 0:\n",
    "k = 2  # some history slot, not the last\n",
    "prev_states_pert[0, k] += 1.5\n",
    "next_states_pert[0,  k] -= 1.0\n",
    "prev_actions_pert[0, k] = (prev_actions_pert[0, k] + 1) % action_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_pert = m(prev_states_pert, prev_actions_pert, next_states_pert, slot_idx, timestep, action_mask_last=action_mask_last)\n",
    "    probs_pert  = F.softmax(logits_pert, dim=-1)\n",
    "\n",
    "print(\"Probs after history change (batch 0):\", probs_pert[0].tolist())\n",
    "\n",
    "# Compare the difference for batch 0\n",
    "delta = (probs[0] - probs_pert[0]).abs()\n",
    "print(\"Max abs change in probs (batch 0):\", float(delta.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy actions (per batch): [43, 2]\n",
      "Sampled actions (per batch): [114, 103]\n"
     ]
    }
   ],
   "source": [
    "greedy_actions = probs.argmax(dim=-1)              # shape: (B,)\n",
    "\n",
    "# OR: Sampling (stochastic) action per batch\n",
    "sampled_actions = torch.multinomial(probs, num_samples=1).squeeze(-1)  # shape: (B,)\n",
    "\n",
    "# If your downstream code needs CPU ints:\n",
    "greedy_actions_cpu   = greedy_actions.detach().cpu().tolist()   # e.g., [37, 5]\n",
    "sampled_actions_cpu  = sampled_actions.detach().cpu().tolist()  # e.g., [12, 88]\n",
    "\n",
    "print(\"Greedy actions (per batch):\", greedy_actions_cpu)\n",
    "print(\"Sampled actions (per batch):\", sampled_actions_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Greedy actions: [99, 63, 32, 32, 88, 44, 87, 87, 135, 44, 149, 33, 32, 33, 32, 167, 44, 165, 44, 144, 87, 32, 87, 32, 53, 32, 33, 32, 167, 135, 87, 32]\n",
      "Sampled actions: [41, 162, 48, 96, 14, 18, 111, 136, 69, 158, 88, 119, 104, 14, 25, 21, 80, 67, 154, 75, 140, 173, 14, 21, 30, 40, 145, 84, 59, 150, 16, 44]\n",
      "Last-step probs (first 10): [0.006164961028844118, 0.004137567710131407, 0.0048055206425487995, 0.006171464454382658, 0.004117234144359827, 0.005092212464660406, 0.007607095409184694, 0.005709832068532705, 0.005305347964167595, 0.005209771450608969]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# --------- device ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --------- your model class must already be defined (gridGPT, Block, etc.) ----------\n",
    "m = gridGPT().to(device).eval()\n",
    "\n",
    "# --------- config ----------\n",
    "L            = m.config['context_len']   # window length\n",
    "state_dim    = m.config['state_dim']\n",
    "action_size  = m.config['action_size']\n",
    "max_timestep = m.config['max_timestep']\n",
    "\n",
    "# --------- dummy environment ----------\n",
    "# Very simple linear-ish dynamics: s_{t+1} = s_t + W[:, a] * 0.05 + noise\n",
    "# (so actions nudge state in different directions)\n",
    "g = torch.Generator(device=device).manual_seed(7)\n",
    "W = torch.randn(state_dim, action_size, generator=g, device=device) * 0.1\n",
    "\n",
    "def env_step(state, action_id, g):\n",
    "    # Create noise with the same shape/device/dtype as `state`\n",
    "    noise = torch.randn(state.shape, device=state.device, dtype=state.dtype, generator=g) * 0.01\n",
    "    drift = (W[:, action_id].to(state.dtype)) * 0.05\n",
    "    return state + drift + noise\n",
    "\n",
    "\n",
    "# --------- rollout length (<= max_timestep - 1 to be safe) ----------\n",
    "T = min(32, max_timestep - 1)  # number of decisions we’ll take\n",
    "\n",
    "# --------- buffers for the sliding window (B=1) ----------\n",
    "# We’ll maintain the window content and shift left each step.\n",
    "window_prev_states = torch.zeros(L, state_dim, device=device)\n",
    "window_next_states = torch.zeros(L, state_dim, device=device)\n",
    "window_actions     = torch.zeros(L, dtype=torch.long, device=device)  # pad with action 0\n",
    "\n",
    "# slot indices are always 0..L-1\n",
    "slot_idx_full = torch.arange(L, device=device).unsqueeze(0)  # [1, L]\n",
    "\n",
    "# mask: all actions valid at the last slot\n",
    "action_mask_last = torch.ones(1, action_size, dtype=torch.bool, device=device)\n",
    "\n",
    "# --------- initial state s0 ----------\n",
    "s_t = torch.randn(state_dim, generator=g, device=device) * 0.1  # small random start\n",
    "a_prev = 0  # pad action for the first decision\n",
    "\n",
    "greedy_actions = []\n",
    "sampled_actions = []\n",
    "probs_history = []\n",
    "\n",
    "for t in range(T):\n",
    "    # Shift-left the window by 1 (discard oldest slot)\n",
    "    window_prev_states = torch.roll(window_prev_states, shifts=-1, dims=0)\n",
    "    window_next_states = torch.roll(window_next_states, shifts=-1, dims=0)\n",
    "    window_actions     = torch.roll(window_actions,     shifts=-1, dims=0)\n",
    "\n",
    "    # For slot L-1 (the \"decision\" slot):\n",
    "    # prev_state = s_{t-1} if t>0 else zeros\n",
    "    if t == 0:\n",
    "        prev_slot = torch.zeros(state_dim, device=device)\n",
    "        last_action = 0  # pad\n",
    "    else:\n",
    "        prev_slot = s_prev.clone()\n",
    "        last_action = int(a_prev)\n",
    "\n",
    "    # next_state = s_t (current observed state)\n",
    "    window_prev_states[-1] = prev_slot\n",
    "    window_next_states[-1] = s_t\n",
    "    window_actions[-1]     = last_action\n",
    "\n",
    "    # Build batch tensors [B=1, L, ...]\n",
    "    prev_states_b = window_prev_states.unsqueeze(0)  # [1, L, state_dim]\n",
    "    next_states_b = window_next_states.unsqueeze(0)  # [1, L, state_dim]\n",
    "    actions_b     = window_actions.unsqueeze(0)      # [1, L]\n",
    "\n",
    "    # Absolute time indices for positions in the window.\n",
    "    # We'll map the k-th slot to time (t-L+1+k), clipped to [0, max_timestep-1]\n",
    "    base = t - (L - 1)\n",
    "    times = torch.clamp(torch.arange(base, base + L, device=device), min=0, max=max_timestep-1)\n",
    "    timestep_b = times.unsqueeze(0)  # [1, L]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = m(\n",
    "            prev_states_b, actions_b, next_states_b,\n",
    "            slot_idx=slot_idx_full, timestep=timestep_b,\n",
    "            action_mask_last=action_mask_last\n",
    "        )  # [1, action_size]\n",
    "        probs = F.softmax(logits, dim=-1)  # [1, action_size]\n",
    "\n",
    "    # Choose action for time t\n",
    "    a_greedy  = int(probs.argmax(dim=-1)[0].item())\n",
    "    a_sampled = int(torch.multinomial(probs[0], num_samples=1).item())\n",
    "\n",
    "    greedy_actions.append(a_greedy)\n",
    "    sampled_actions.append(a_sampled)\n",
    "    probs_history.append(probs[0].detach().cpu())\n",
    "\n",
    "    # Take the action (choose greedy by default here)\n",
    "    a_t = a_greedy\n",
    "\n",
    "    # Step env: produce s_{t+1}\n",
    "    s_prev = s_t.clone()\n",
    "    s_t = env_step(s_t, a_t, g)\n",
    "\n",
    "\n",
    "    # Remember for next loop\n",
    "    a_prev = a_t\n",
    "\n",
    "# --------- results ----------\n",
    "print(\"\\nGreedy actions:\", greedy_actions)\n",
    "print(\"Sampled actions:\", sampled_actions)\n",
    "print(\"Last-step probs (first 10):\", probs_history[-1][:10].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2rpn-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
